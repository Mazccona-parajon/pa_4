---
title: 'Programming assignment 4'
output: github_document
---

**Author**: Maricruz Azcona-Parajon  
**Date**: Last update: `r Sys.time()`

# Overview

For this assignment, I analyze productions of Spanish-like nonce words produced by bilingual speakers and L2 learners. The targets are paroxytones with voiceless stops and all five Spanish oral vowels.

**Hypothesis.** Because these recordings are productions of Spanish-like nonce words containing voiceless stops and all five Spanish oral vowels, bilingual speakers will produce more target-like Spanish segmental patterns than the L2 learners. Specifically, bilinguals are expected to show shorter, more consistent VOT values for /p t k/ and clearer, more peripheral vowel formant patterns in F1â€“F2 space. In contrast, L2 learners are expected to produce longer, more English-like VOTs and more centralized, less distinct vowel qualities, with greater variability across repetitions. The vowels for each speaker should reflect that speaker's dominant language. Overall, the acoustic measures of VOT and formants should reveal more native-like stability in the bilinguals and more L1-influenced variability in the L2 learners.

# Prep

## Libraries

```{r}
#| label: install-libs
#| echo: false
#| warning: false
#| message: false

# Install packages if not already installed
required_packages <- c("tidyverse", "here", "knitr")

for (pkg in required_packages) {
  if (!require(pkg, character.only = TRUE, quietly = TRUE)) {
    install.packages(pkg, repos = "https://cloud.r-project.org/")
    library(pkg, character.only = TRUE)
  }
}
```

```{r}
#| label: load-libs
#| echo: false 
#| warning: false 
#| message: false

# Load libraries here
library(tidyverse)
library(here)
library(knitr)
```

## Load data

```{r}
#| label: load-data

# Get all CSV files from the data directory
# Your structure: Desktop/pa_4/data/
data_files <- list.files(path = here("data"), 
                         pattern = "\\.csv$", 
                         full.names = TRUE)

# Read and combine all data files
# This will combine: bi01.csv, bi02.csv, bi03.csv, ne01.csv, ne02.csv, ne03.csv
df <- data_files %>%
  map_df(read_csv, show_col_types = FALSE)

# If you want to keep track of which file each row came from:
# df <- data_files %>%
#   map_df(~ read_csv(.x, show_col_types = FALSE) %>%
#            mutate(file = basename(.x)), .id = "source")
```

## Tidy data

```{r}
#| label: tidy-data

# Create a participant group variable based on filename
# bi = bilingual, ne = L2 learner
df <- df %>%
  separate(fileID, into = c("id", "item"), sep = "_") |> 
  mutate(
    group = if_else(str_detect(id, "bi"), "bilingual", "native_en"), 
    vowel = case_when(
      str_detect(item, "i") ~ "i", 
      str_detect(item, "e") ~ "e", 
      str_detect(item, "a") ~ "a", 
      str_detect(item, "o") ~ "o", 
      str_detect(item, "u") ~ "u"
    )
  )

# If you need to convert from wide to long format for vowel formants:
# df_long <- df %>%
#   pivot_longer(cols = c(f1, f2, f3), 
#                names_to = "formant", 
#                values_to = "hz")

# If you need separate variables for consonant and vowel:
# df <- df %>%
#   mutate(
#     consonant = str_extract(item, "[ptk]"),
#     vowel = str_extract(item, "[aeiou]")
#   )
```

# Analysis

## Descriptives

```{r}
#| label: descriptive-stats

# VOT descriptive statistics by group
vot_stats <- df %>%
  group_by(group) %>%
  summarise(
    n = n(),
    mean_vot = mean(vot, na.rm = TRUE),
    sd_vot = sd(vot, na.rm = TRUE),
    min_vot = min(vot, na.rm = TRUE),
    max_vot = max(vot, na.rm = TRUE)
  )

kable(vot_stats, 
      digits = 2,
      caption = "VOT Descriptive Statistics by Speaker Group")

# Formant descriptive statistics by group and vowel
formant_stats <- df %>%
  group_by(group, vowel) %>%
  summarise(
    n = n(),
    mean_f1 = mean(f1, na.rm = TRUE),
    sd_f1 = sd(f1, na.rm = TRUE),
    mean_f2 = mean(f2, na.rm = TRUE),
    sd_f2 = sd(f2, na.rm = TRUE),
    .groups = "drop"
  )

kable(formant_stats, 
      digits = 2,
      caption = "Formant Descriptive Statistics by Group and Vowel")
```

## Visualization

```{r}
#| label: plots 
#| fig.retina: 2

# VOT by group
ggplot(df, aes(x = group, y = vot, fill = group)) +
  geom_boxplot(alpha = 0.7) +
  geom_jitter(width = 0.2, alpha = 0.3) +
  theme_minimal() +
  labs(title = "VOT Distribution by Speaker Group",
       x = "Speaker Group",
       y = "VOT (ms)") +
  theme(legend.position = "none")

# Vowel space plot (F1 vs F2)
ggplot(df, aes(x = f2, y = f1, color = vowel, shape = group)) +
  geom_point(size = 3, alpha = 0.6) +
  scale_x_reverse() +
  scale_y_reverse() +
  theme_minimal() +
  labs(title = "Vowel Space: F1 vs F2 by Group",
       x = "F2 (Hz)",
       y = "F1 (Hz)",
       color = "Vowel",
       shape = "Group")
```

Figure 1: Example acoustic analysis from Praat

```{r}
#| label: praat-figure
#| echo: false
#| out.width: "80%"
#| fig.align: "center"

# Include Praat-generated figure
# Assumes image is in 'figs' or 'figures' folder
knitr::include_graphics(here("figs", "praat_spectrogram.png"))

# Alternative paths depending on your project structure:
# knitr::include_graphics(here("figures", "praat_analysis.png"))
# knitr::include_graphics(here("images", "acoustic_plot.png"))
```

## Hypothesis test

```{r}
#| label: stats

# Test for VOT differences between groups
vot_model <- lm(vot ~ group, data = df)
summary(vot_model)

# Alternative: t-test for VOT
# vot_test <- t.test(vot ~ group, data = df)
# print(vot_test)

# Test for formant differences (F1)
f1_model <- lm(f1 ~ group * vowel, data = df)
summary(f1_model)

# Test for formant differences (F2)
f2_model <- lm(f2 ~ group * vowel, data = df)
summary(f2_model)
```

# Conclusion

In conclusion, the results partially supported the hypothesis that bilingual speakers would produce more target-like Spanish patterns than L2 learners, with bilinguals generally showing shorter and more consistent VOT values and clearer vowel spaces. L2 learners showed greater variability and, in some cases, more English-like patterns in both VOT and vowel quality. This project was more challenging than expected due to repeated coding errors, issues with the vowel data, and the need to rely on ChatGPT and google to help with R. However, despite the frustration, and many hours of segmentation and having to walk away from my computer, the experience highlighted how much real research involves troubleshooting and taught me the importance of just pushing through and learning from my mistakes (I deleted my code like 6 times.)
</br></br>
